{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDH2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pryplotsky/C5.-Deep-Survival-Analysis-and-Time-Varying-Covariates/blob/main/DDH2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lF31EnFuVmT"
      },
      "source": [
        "Instruction:\n",
        "1. Fork DDH repository to your gihub \n",
        "2. Clone DDH repository to google colab (or python)\n",
        "3. Import additional packages\n",
        "\n",
        "When I cloned DDH repository to colab, I tryed to run tutorial.ipynb from DDH repository, but I had an error in: from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
        "(ModuleNotFoundError: No module named 'tensorflow.contrib')\n",
        "istalling additional packages I fixed it, but now we should integarte \"Model_Longitudinal_Attention\" in our code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFAqmZOyZPNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76971541-15d6-4ee1-c436-d0688ff3cff2"
      },
      "source": [
        "!git clone https://github.com/pryplotsky/Dynamic-DeepHit.git #clone github modules\n",
        "import sys #set directory for modules \n",
        "sys.path.insert(0,'/content/Dynamic-DeepHit')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Dynamic-DeepHit'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 6), reused 17 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIR-CH1Xj14p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394fbccd-0476-4ab4-86e9-e8081aa272a9"
      },
      "source": [
        "!pip install lifelines #install required package\n",
        "from  keras.layers import Dense as FC_Net #fix bag with tensorflow.contrib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lifelines\n",
            "  Downloading lifelines-0.26.0-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
            "Collecting formulaic<0.3,>=0.2.2\n",
            "  Downloading formulaic-0.2.4-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Collecting interface-meta>=1.2\n",
            "  Downloading interface_meta-1.2.3-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=e3b410805c0e081127733668169abbbc65d4480f469ecab3641383b99975cd1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, formulaic, autograd-gamma, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-0.2.4 interface-meta-1.2.3 lifelines-0.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAlR7se3gTkF"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "#from tensorflow.contrib.layers import fully_connected as FC_Net\n",
        "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
        "\n",
        "\n",
        "#import utils_network as utils\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    return tf.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.div(x, (y + _EPSILON))\n",
        "\n",
        "def get_seq_length(sequence):\n",
        "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
        "    tmp_length = tf.reduce_sum(used, 1)\n",
        "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
        "    return tmp_length\n",
        "\n",
        "\n",
        "class Model_Longitudinal_Attention:\n",
        "    # def __init__(self, sess, name, mb_size, input_dims, network_settings):\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "\n",
        "        # INPUT DIMENSIONS\n",
        "        self.x_dim              = input_dims['x_dim']\n",
        "        self.x_dim_cont         = input_dims['x_dim_cont']\n",
        "        self.x_dim_bin          = input_dims['x_dim_bin']\n",
        "\n",
        "        self.num_Event          = input_dims['num_Event']\n",
        "        self.num_Category       = input_dims['num_Category']\n",
        "        self.max_length         = input_dims['max_length']\n",
        "\n",
        "        # NETWORK HYPER-PARMETERS\n",
        "        self.h_dim1             = network_settings['h_dim_RNN']\n",
        "        self.h_dim2             = network_settings['h_dim_FC']\n",
        "        self.num_layers_RNN     = network_settings['num_layers_RNN']\n",
        "        self.num_layers_ATT     = network_settings['num_layers_ATT']\n",
        "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
        "\n",
        "        self.RNN_type           = network_settings['RNN_type']\n",
        "\n",
        "        self.FC_active_fn       = network_settings['FC_active_fn']\n",
        "        self.RNN_active_fn      = network_settings['RNN_active_fn']\n",
        "        self.initial_W          = network_settings['initial_W']\n",
        "        \n",
        "        self.reg_W              = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W'])\n",
        "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W_out'])\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            #### PLACEHOLDER DECLARATION\n",
        "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "\n",
        "            self.lr_rate     = tf.placeholder(tf.float32)\n",
        "            self.keep_prob   = tf.placeholder(tf.float32)                                                      #keeping rate\n",
        "            self.a           = tf.placeholder(tf.float32)\n",
        "            self.b           = tf.placeholder(tf.float32)\n",
        "            self.c           = tf.placeholder(tf.float32)\n",
        "\n",
        "            self.x           = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])\n",
        "            self.x_mi        = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])           #this is the missing indicator (including for cont. & binary) (includes delta)\n",
        "            self.k           = tf.placeholder(tf.float32, shape=[None, 1])                                     #event/censoring label (censoring:0)\n",
        "            self.t           = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "\n",
        "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for denominator\n",
        "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for Loss 1\n",
        "            self.fc_mask3    = tf.placeholder(tf.float32, shape=[None, self.num_Category])                     #for Loss 2\n",
        "\n",
        "            \n",
        "            seq_length     = get_seq_length(self.x)\n",
        "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
        "            \n",
        "            self.rnn_mask1 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32)            \n",
        "            self.rnn_mask2 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32) \n",
        "            \n",
        "            \n",
        "            ### DEFINE LOOP FUNCTION FOR RAW_RNN w/ TEMPORAL ATTENTION\n",
        "            def loop_fn_att(time, cell_output, cell_state, loop_state):\n",
        "\n",
        "                emit_output = cell_output \n",
        "\n",
        "                if cell_output is None:  # time == 0\n",
        "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
        "                    next_loop_state = loop_state_ta\n",
        "                else:\n",
        "                    next_cell_state = cell_state\n",
        "                    tmp_h = utils.create_concat_state(next_cell_state, self.num_layers_RNN, self.RNN_type)\n",
        "\n",
        "                    e = utils.create_FCNet(tf.concat([tmp_h, all_last], axis=1), self.num_layers_ATT, self.h_dim2, \n",
        "                                           tf.nn.tanh, 1, None, self.initial_W, keep_prob=self.keep_prob)\n",
        "                    e = tf.exp(e)\n",
        "\n",
        "                    next_loop_state = (loop_state[0].write(time-1, e),                # save att power (e_{j})\n",
        "                                       loop_state[1].write(time-1, tmp_h))  # save all the hidden states\n",
        "\n",
        "                # elements_finished = (time >= seq_length)\n",
        "                elements_finished = (time >= self.max_length-1)\n",
        "\n",
        "                #this gives the break-point (no more recurrence after the max_length)\n",
        "                finished = tf.reduce_all(elements_finished)    \n",
        "                next_input = tf.cond(finished, lambda: tf.zeros([self.mb_size, 2*self.x_dim], dtype=tf.float32),  # [x_hist, mi_hist]\n",
        "                                               lambda: inputs_ta.read(time))\n",
        "\n",
        "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
        "\n",
        "\n",
        "            \n",
        "            # divide into the last x and previous x's\n",
        "            x_last = tf.slice(self.x, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
        "            x_last = tf.reshape(x_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
        "\n",
        "            x_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
        "            x_last = tf.slice(x_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
        "            x_hist = self.x * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
        "            x_hist = tf.slice(x_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
        "\n",
        "            # do same thing for missing indicator\n",
        "            mi_last = tf.slice(self.x_mi, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
        "            mi_last = tf.reshape(mi_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
        "\n",
        "            mi_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x_mi, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
        "            mi_last = tf.slice(mi_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
        "            mi_hist = self.x_mi * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
        "            mi_hist = tf.slice(mi_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
        "\n",
        "            all_hist = tf.concat([x_hist, mi_hist], axis=2)\n",
        "            all_last = tf.concat([x_last, mi_last], axis=1)\n",
        "\n",
        "\n",
        "            #extract inputs for the temporal attention: mask (to incorporate only the measured time) and x_{M}\n",
        "            seq_length     = get_seq_length(x_hist)\n",
        "            rnn_mask_att   = tf.cast(tf.not_equal(tf.reduce_sum(x_hist, reduction_indices=2), 0), dtype=tf.float32)  #[mb_size, max_length-1], 1:measurements 0:no measurements\n",
        "            \n",
        "\n",
        "            ##### SHARED SUBNETWORK: RNN w/ TEMPORAL ATTENTION\n",
        "            #change the input tensor to TensorArray format with [max_length, mb_size, x_dim]\n",
        "            inputs_ta = tf.TensorArray(dtype=tf.float32, size=self.max_length-1).unstack(_transpose_batch_time(all_hist), name = 'Shared_Input')\n",
        "\n",
        "\n",
        "            #create a cell with RNN hyper-parameters (RNN types, #layers, #nodes, activation functions, keep proability)\n",
        "            cell = utils.create_rnn_cell(self.h_dim1, self.num_layers_RNN, self.keep_prob, \n",
        "                                         self.RNN_type, self.RNN_active_fn)\n",
        "\n",
        "            #define the loop_state TensorArray for information from rnn time steps\n",
        "            loop_state_ta = (tf.TensorArray(size=self.max_length-1, dtype=tf.float32),  #e values (e_{j})\n",
        "                             tf.TensorArray(size=self.max_length-1, dtype=tf.float32))  #hidden states (h_{j})\n",
        "            \n",
        "            rnn_outputs_ta, self.rnn_final_state, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn_att)\n",
        "            #rnn_outputs_ta  : TensorArray\n",
        "            #rnn_final_state : Tensor\n",
        "            #rnn_states_ta   : (TensorArray, TensorArray)\n",
        "\n",
        "            rnn_outputs = _transpose_batch_time(rnn_outputs_ta.stack())\n",
        "            # rnn_outputs =  tf.reshape(rnn_outputs, [-1, self.max_length-1, self.h_dim1])\n",
        "\n",
        "            rnn_states  = _transpose_batch_time(loop_state_ta[1].stack())\n",
        "\n",
        "            att_weight  = _transpose_batch_time(loop_state_ta[0].stack()) #e_{j}\n",
        "            att_weight  = tf.reshape(att_weight, [-1, self.max_length-1]) * rnn_mask_att # masking to set 0 for the unmeasured e_{j}\n",
        "\n",
        "            #get a_{j} = e_{j}/sum_{l=1}^{M-1}e_{l}\n",
        "            self.att_weight  = div(att_weight,(tf.reduce_sum(att_weight, axis=1, keepdims=True) + _EPSILON)) #softmax (tf.exp is done, previously)\n",
        "\n",
        "            # 1) expand att_weight to hidden state dimension, 2) c = \\sum_{j=1}^{M} a_{j} x h_{j}\n",
        "            self.context_vec = tf.reduce_sum(tf.tile(tf.reshape(self.att_weight, [-1, self.max_length-1, 1]), [1, 1, self.num_layers_RNN*self.h_dim1]) * rnn_states, axis=1)\n",
        "\n",
        "\n",
        "            self.z_mean      = FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_mean1\")\n",
        "            self.z_std       = tf.exp(FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_std1\"))\n",
        "\n",
        "            epsilon          = tf.random_normal([self.mb_size, self.max_length-1, self.x_dim], mean=0.0, stddev=1.0, dtype=tf.float32)\n",
        "            self.z           = self.z_mean + self.z_std * epsilon\n",
        "\n",
        "            \n",
        "            ##### CS-SPECIFIC SUBNETWORK w/ FCNETS \n",
        "            inputs = tf.concat([x_last, self.context_vec], axis=1)\n",
        "\n",
        "\n",
        "            #1 layer for combining inputs\n",
        "            h = FC_Net(inputs, self.h_dim2, activation_fn=self.FC_active_fn, weights_initializer=self.initial_W, scope=\"Layer1\")\n",
        "            h = tf.nn.dropout(h, keep_prob=self.keep_prob)\n",
        "\n",
        "            # (num_layers_CS-1) layers for cause-specific (num_Event subNets)\n",
        "            out = []\n",
        "            for _ in range(self.num_Event):\n",
        "                cs_out = utils.create_FCNet(h, (self.num_layers_CS), self.h_dim2, self.FC_active_fn, self.h_dim2, self.FC_active_fn, self.initial_W, self.reg_W, self.keep_prob)\n",
        "                out.append(cs_out)\n",
        "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
        "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim2])\n",
        "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
        "\n",
        "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
        "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
        "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
        "\n",
        "\n",
        "            ##### GET LOSS FUNCTIONS\n",
        "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
        "            self.loss_Ranking()             #get loss2: Ranking loss\n",
        "            self.loss_RNN_Prediction()      #get loss3: RNN prediction loss\n",
        "\n",
        "            self.LOSS_TOTAL     = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3 + tf.losses.get_regularization_loss()\n",
        "            self.LOSS_BURNIN    = self.LOSS_3 + tf.losses.get_regularization_loss()\n",
        "\n",
        "            self.solver         = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
        "            self.solver_burn_in = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_BURNIN)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
        "    def loss_Log_Likelihood(self):\n",
        "        sigma3 = tf.constant(1.0, dtype=tf.float32)\n",
        "\n",
        "        I_1 = tf.sign(self.k)\n",
        "        denom = 1 - tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True) # make subject specific denom.\n",
        "        denom = tf.clip_by_value(denom, tf.cast(_EPSILON, dtype=tf.float32), tf.cast(1.-_EPSILON, dtype=tf.float32))\n",
        "\n",
        "        #for uncenosred: log P(T=t,K=k|x,Y,t>t_M)\n",
        "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp1 = I_1 * log(div(tmp1,denom))\n",
        "\n",
        "        #for censored: log \\sum P(T>t|x,Y,t>t_M)\n",
        "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp2 = (1. - I_1) * log(div(tmp2,denom))\n",
        "\n",
        "        self.LOSS_1 = - tf.reduce_mean(tmp1 + sigma3*tmp2)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
        "    def loss_Ranking(self):\n",
        "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
        "\n",
        "        eta = []\n",
        "        for e in range(self.num_Event):\n",
        "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
        "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
        "            I_2 = tf.diag(tf.squeeze(I_2))\n",
        "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
        "\n",
        "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask3)) #no need to divide by each individual dominator\n",
        "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
        "\n",
        "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
        "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
        "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
        "\n",
        "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
        "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
        "\n",
        "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
        "\n",
        "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keepdims=True)\n",
        "\n",
        "            eta.append(tmp_eta)\n",
        "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
        "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keepdims=True)\n",
        "\n",
        "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 3 -- RNN prediction loss\n",
        "    def loss_RNN_Prediction(self):\n",
        "        tmp_x  = tf.slice(self.x, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
        "        tmp_mi = tf.slice(self.x_mi, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
        "\n",
        "        tmp_mask1  = tf.tile(tf.expand_dims(self.rnn_mask1, axis=2), [1,1,self.x_dim]) #for hisotry (1...J-1)\n",
        "        tmp_mask1  = tmp_mask1[:, :(self.max_length-1), :] \n",
        "\n",
        "        zeta = tf.reduce_mean(tf.reduce_sum(tmp_mask1 * (1. - tmp_mi) * tf.pow(self.z - tmp_x, 2), reduction_indices=1))  #loss calculated for selected features.\n",
        "\n",
        "        self.LOSS_3 = zeta\n",
        "\n",
        " \n",
        "    def get_cost(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
        "        (x_mi_mb)                 = MISSING\n",
        "        (alpha, beta, gamma)      = PARAMETERS\n",
        "        return self.sess.run(self.LOSS_TOTAL, \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
        "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
        "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "\n",
        "    def train(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
        "        (x_mi_mb)                 = MISSING\n",
        "        (alpha, beta, gamma)      = PARAMETERS\n",
        "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb,\n",
        "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
        "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def train_burn_in(self, DATA, MISSING, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (x_mi_mb)                 = MISSING\n",
        "\n",
        "        return self.sess.run([self.solver_burn_in, self.LOSS_3], \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def predict(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_z(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.z, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_rnnstate(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.rnn_final_state, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_att(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.att_weight, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_context_vec(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.context_vec, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def get_z_mean_and_std(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run([self.z_mean, self.z_std], feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JpB7u5yZN4k"
      },
      "source": [
        "_EPSILON = 1e-08\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import import_data as impt\n",
        "#from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
        "\n",
        "\n",
        "from utils_eval             import c_index, brier_score\n",
        "from utils_log              import save_logging, load_logging\n",
        "from utils_helper           import f_get_minibatch, f_get_boosted_trainset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmGYt2ww10-i"
      },
      "source": [
        "\n",
        "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(data))\n",
        "    new_data_mi = np.zeros(np.shape(data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)\n",
        "\n",
        "\n",
        "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
        "    \n",
        "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
        "    _, num_Event, num_Category = np.shape(pred)\n",
        "       \n",
        "    risk_all = {}\n",
        "    for k in range(num_Event):\n",
        "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
        "            \n",
        "    for p, p_time in enumerate(pred_time):\n",
        "        ### PREDICTION\n",
        "        pred_horizon = int(p_time)\n",
        "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
        "\n",
        "\n",
        "        for t, t_time in enumerate(eval_time):\n",
        "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
        "\n",
        "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
        "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
        "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
        "            \n",
        "            for k in range(num_Event):\n",
        "                risk_all[k][:, p, t] = risk[:, k]\n",
        "                \n",
        "    return risk_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS9wWqH17wB"
      },
      "source": [
        "data_mode                   = 'PBC2' \n",
        "seed                        = 1234\n",
        "\n",
        "##### IMPORT DATASET\n",
        "'''\n",
        "    num_Category            = max event/censoring time * 1.2\n",
        "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
        "    max_length              = maximum number of measurements\n",
        "    x_dim                   = data dimension including delta (1 + num_features)\n",
        "    x_dim_cont              = dim of continuous features\n",
        "    x_dim_bin               = dim of binary features\n",
        "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
        "'''\n",
        "\n",
        "if data_mode == 'PBC2':\n",
        "    (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = impt.import_dataset(norm_mode = 'standard')\n",
        "    \n",
        "    # This must be changed depending on the datasets, prediction/evaliation times of interest\n",
        "    pred_time = [52, 3*52, 5*52] # prediction time (in months)\n",
        "    eval_time = [12, 36, 60, 120] # months evaluation time (for C-index and Brier-Score)\n",
        "else:\n",
        "    print ('ERROR:  DATA_MODE NOT FOUND !!!')\n",
        "\n",
        "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
        "max_length                  = np.shape(data)[1]\n",
        "\n",
        "\n",
        "file_path = '{}'.format(data_mode)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    os.makedirs(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5CzafIPzGU4"
      },
      "source": [
        "burn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
        "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
        "\n",
        "##### HYPER-PARAMETERS\n",
        "new_parser = {'mb_size': 32,\n",
        "\n",
        "             'iteration_burn_in': 3000,\n",
        "             'iteration': 25000,\n",
        "\n",
        "             'keep_prob': 0.6,\n",
        "             'lr_train': 1e-4,\n",
        "\n",
        "             'h_dim_RNN': 100,\n",
        "             'h_dim_FC' : 100,\n",
        "             'num_layers_RNN':2,\n",
        "             'num_layers_ATT':2,\n",
        "             'num_layers_CS' :2,\n",
        "\n",
        "             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n",
        "\n",
        "             'FC_active_fn' : tf.nn.relu,\n",
        "             'RNN_active_fn': tf.nn.tanh,\n",
        "\n",
        "            'reg_W'         : 1e-5,\n",
        "            'reg_W_out'     : 0.,\n",
        "\n",
        "             'alpha' :1.0,\n",
        "             'beta'  :0.1,\n",
        "             'gamma' :1.0\n",
        "}\n",
        "# INPUT DIMENSIONS\n",
        "input_dims                  = { 'x_dim'         : x_dim,\n",
        "                                'x_dim_cont'    : x_dim_cont,\n",
        "                                'x_dim_bin'     : x_dim_bin,\n",
        "                                'num_Event'     : num_Event,\n",
        "                                'num_Category'  : num_Category,\n",
        "                                'max_length'    : max_length }\n",
        "\n",
        "# NETWORK HYPER-PARMETERS\n",
        "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
        "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
        "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
        "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
        "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
        "                                'RNN_type'          : new_parser['RNN_type'],\n",
        "                                'FC_active_fn'      : new_parser['FC_active_fn'],\n",
        "                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n",
        "                                'initial_W'         : tf.keras.initializers.GlorotNormal(seed=None),\n",
        "\n",
        "                                'reg_W'             : new_parser['reg_W'],\n",
        "                                'reg_W_out'         : new_parser['reg_W_out']\n",
        "                                 }\n",
        "\n",
        "\n",
        "mb_size           = new_parser['mb_size']\n",
        "iteration         = new_parser['iteration']\n",
        "iteration_burn_in = new_parser['iteration_burn_in']\n",
        "\n",
        "keep_prob         = new_parser['keep_prob']\n",
        "lr_train          = new_parser['lr_train']\n",
        "\n",
        "alpha             = new_parser['alpha']\n",
        "beta              = new_parser['beta']\n",
        "gamma             = new_parser['gamma']\n",
        "# SAVE HYPERPARAMETERS\n",
        "log_name = file_path + '/hyperparameters_log.txt'\n",
        "save_logging(new_parser, log_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7INOSqxozgJE"
      },
      "source": [
        "### TRAINING-TESTING SPLIT\n",
        "(tr_data,te_data, tr_data_mi, te_data_mi, tr_time,te_time, tr_label,te_label, \n",
        " tr_mask1,te_mask1, tr_mask2,te_mask2, tr_mask3,te_mask3) = train_test_split(data, data_mi, time, label, mask1, mask2, mask3, test_size=0.2, random_state=seed) \n",
        "\n",
        "(tr_data,va_data, tr_data_mi, va_data_mi, tr_time,va_time, tr_label,va_label, \n",
        " tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size=0.2, random_state=seed) \n",
        "\n",
        "if boost_mode == 'ON':\n",
        "    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "VToK2F11znGD",
        "outputId": "e66c7890-a566-4b88-d0cb-901b6037b43a"
      },
      "source": [
        "##### CREATE DYNAMIC-DEEPFHT NETWORK\n",
        "#tf.reset_default_graph()\n",
        "\n",
        "#config = tf.config()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#sess = tf.Session(config=config)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    print(e)\n",
        "\n",
        "#model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
        "model = Model_Longitudinal_Attention(gpus, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-47f8c33b9808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Longitudinal_Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dyanmic-DeepHit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-77255aad50b6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, name, input_dims, network_settings)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_W\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mnetwork_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_W\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reg_W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_W_out\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reg_W_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ]
    }
  ]
}