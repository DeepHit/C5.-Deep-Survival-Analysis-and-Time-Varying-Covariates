# -*- coding: utf-8 -*-
"""Error.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/pryplotsky/C5.-Deep-Survival-Analysis-and-Time-Varying-Covariates/blob/main/Error.ipynb
"""

#Some basic istructions:
# 1. Each line of code must be FULLY DOCUMENTED
# 2. Everything should be inside a class(classes) or functions if you don't know OOP
# 3. Please avoid using values (numbers/string/....) use instead variables
# 4. Use default values for your function arguments whenever possible

#Plan:
# We need functions/methods for:
# 1. Read, save and load a data - DONE
# 2. Pre-process the data (if needed) - DONE
# 3. Split the data into 3 sets: train(80%)  + dev (10%) + test(10%) - DONE
# 4. Obtain relevant statistics (if needed) - NOT NOW
# 5. Create Cox time ( CPH for TVC) model - To discuss
# 6. Plot results - To discuss

### Install  packages and define global variables
#Basic:
import pandas as pd
import numpy as np
# 1. Read, save and load a data:
import os
import pickle
# 2. Pre-process the data 
from sklearn import preprocessing
# 3. Split the data into 3 sets
import sklearn
from sklearn.model_selection import train_test_split
# 5. Create Cox time ( CPH for TVC) model
!pip install lifelines
from lifelines import CoxTimeVaryingFitter


import matplotlib.pyplot as plt

"""**Main part of code**"""

### Create class Preparation with 5 methods: __init__, readdf, save, load, cleaning and splitdata
class Preparation:
  def __init__(self):# Dont use any additional attributes    
    self.df = None # Placeholders. Set varabels as none and then update a resut 
    self.X_train = None
    self.X_test = None
    self.y_train = None
    self.y_test  = None
    self.X_train = None
    self.X_val = None
    self.y_train = None
    self.y_val = None
    self.cols_standardize = ['rate_time','hpi_time', 'gdp_time']# place for variables to standartize 
### Pre-processing:
  # Read data
  def readdf (self, sep=",", filename="dcr.csv", cwd = os.getcwd()): # Method whith 3 def attributes: sep - separator, getcwd - path to your working directory
    file_name = cwd + "/" + filename # Get path of file
    data = pd.read_csv(file_name, sep= sep) # Read csv
    self.df = data # Save filtered dataset 
    return self.df # Print dataset
  # Clean data
  def cleaning (self, data,longformat=True,individual="id", stop="time", stopname="start"): 
    col1 = data.loc[:,self.cols_standardize]# separate the variables to be standartized
    col2 = data.drop(columns=self.cols_standardize)# separate the variables not to be standartized
    standard = preprocessing.scale(col1)# standardization of variables
    df2 = pd.DataFrame(standard, columns=self.cols_standardize)# to dataframe
    df3=pd.concat([col2,df2], axis=1)# merge 2 df
    data2 = df3.drop_duplicates() #drop double row entries
    self.df = data2 # Save filtered dataset
    if longformat:# Bring data into long format (necessary for using the lifeline package's Coxâ€™s time varying proportional hazard model) 
        data2[stopname] = data2.groupby(individual)[stop].shift(1)
        data2[stopname] = data2[stopname].fillna(0)
        self.df = data2 # Save filtered dataset
        return self.df # Print dataset
### Saving and load objects as binary mode
  def save (self, dataname, dataframe , cwd = os.getcwd()):# Saving and load objects as binary mode
    with open( cwd + '/' + dataname + '.pkl','wb') as path_name: # save df, 'wb' specifies 'write'
      pickle.dump(dataframe, path_name)  
  def load (self, dataname, cwd = os.getcwd()):# Saving and load objects as binary mode
    with open( cwd + '/' + dataname + '.pkl' ,'rb') as path_name:# load df, 'rb' specifies 'read'
      dataframe = pickle.load(path_name)
      return dataframe # Print dataset
### Split the data into 3 sets: train(80%)  + dev (10%) + test(10%)  
  def splitdata (self, Xvar, yvar, perc_test=0.1, perc_val=1/9,  shuffle=False):# Method needs 5 arguments: X set with independent vars, y - set w. dependent vars, perc_test=percentage for test set, perc_val=valuation set (=(1-perc_test)*perc_val) 
    self.X_train, self.X_test, self.y_train, self.y_test = sklearn.model_selection.train_test_split(Xvar, yvar, test_size=perc_test,  shuffle=shuffle)# Split data not randomly to train 90% and test 10%
    self.X_train, self.X_val, self.y_train, self.y_val = sklearn.model_selection.train_test_split(self.X_train, self.y_train, test_size=perc_val,  shuffle=shuffle) # Split train data not randomly to train 80% and valid 10%
    print(len(self.X_train), len(self.X_val), len(self.X_test)) # len - length

"""**Testing**"""

temp=Preparation()
newdf = temp.readdf()
temp.cleaning(newdf)
temp.save('qwer', newdf)
www=temp.load('qwer')
temp.splitdata(temp.df.loc[:,temp.df.columns != "status_time"], yvar=temp.df.status_time)
temp.X_train

"""**Cox Time**"""

base_df=temp.df.loc[:, ["id", "time", "start","rate_time", "hpi_time", "gdp_time", "status_time" ]]# define df with variables for Cox time

base_df.status_time.describe() # Event ( status time has values 0,1,2)

temp.df #full dataframe

# Cox model, code from internet https://lifelines.readthedocs.io/en/latest/Time%20varying%20survival%20regression.html
ctv = CoxTimeVaryingFitter(penalizer=0.1)
ctv.fit(base_df, id_col="id", event_col="status_time", start_col="start", stop_col="time", show_progress=True )
ctv.print_summary() #summary of the fitted model with different values of the parametric partial hazard and Partial AIC

ctv.plot() #visiualizaton of the covariates and how they are distributed

ctv.predict_partial_hazard(base_df) #predicting the partial hazard

#no overfitting???
#how do we actually proceed with these values? In the end we want a prediction for t dont we?

"""# Survival Curve"""

from lifelines import KaplanMeierFitter

survival = KaplanMeierFitter() #
survival.fit(temp.df['time'], temp.df['default_time']) #Survival Curve for our dataset
survival.plot() #plot the Curve
plt.ylabel("Probability of survival")
plt.show()
plt.close()

"""# Concordance Index"""

from lifelines.utils import concordance_index
concordance_index(temp.X_test['time'], ctv.predict_partial_hazard(temp.X_test)) #calculate the C-Index

"""# TVC data split up"""

new_base_df = Preparation
new_base_df.splitdata(new_base_df, Xvar = base_df.loc[:,base_df.columns != "status_time"], yvar = base_df.status_time)

new_base_df.X_train

df2=pd.concat([new_base_df.X_train, new_base_df.y_train], axis=1)

ctv = CoxTimeVaryingFitter(penalizer=0.1)
ctv.fit(df2, id_col="id", event_col="status_time", start_col="start", stop_col="time", show_progress=True )
ctv.print_summary() #summary of the fitted model with different values of the parametric partial hazard and Partial AIC

ctv.plot() #visiualizaton of the covariates and how they are distributed

df3=pd.concat([new_base_df.X_test, new_base_df.y_test], axis=1)

ctv.predict_partial_hazard(df3) #predicting the partial hazard

concordance_index(new_base_df.X_test['time'], ctv.predict_partial_hazard(new_base_df.X_test)) #calculate the C-Index