{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPH001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pryplotsky/C5.-Deep-Survival-Analysis-and-Time-Varying-Covariates/blob/main/CPH001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiTnR5T4YzgG"
      },
      "source": [
        "#Some basic istructions:\n",
        "# 1. Each line of code must be FULLY DOCUMENTED\n",
        "# 2. Everything should be inside a class(classes) or functions if you don't know OOP\n",
        "# 3. Please avoid using values (numbers/string/....) use instead variables\n",
        "# 4. Use default values for your function arguments whenever possible\n",
        "\n",
        "#Plan:\n",
        "# We need functions/methods for:\n",
        "# 1. Read, save and load a data - DONE\n",
        "# 2. Pre-process the data (if needed) - DONE\n",
        "# 3. Split the data into 3 sets: train(80%)  + dev (10%) + test(10%) - DONE\n",
        "# 4. Obtain relevant statistics (if needed) - NOT NOW\n",
        "# 5. Create Cox time ( CPH for TVC) model - IN PROGRESS\n",
        "# 6. Plot results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXIk0aH2daz5"
      },
      "source": [
        "### Install  packages and define global variables\n",
        "#Basic:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# 1. Read, save and load a data:\n",
        "import os\n",
        "import pickle\n",
        "# 2. Pre-process the data \n",
        "from sklearn import preprocessing\n",
        "# 3. Split the data into 3 sets\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Global variables:\n",
        "#...\n",
        "#..."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7BxB1vqeYIB"
      },
      "source": [
        "**Main part of code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4vI-6DjecCr"
      },
      "source": [
        "### Create class Preparation with 5 methods: __init__, readdf, save, load, cleaning and splitdata\n",
        "class Preparation:\n",
        "  def __init__(self):# Dont use any additional attributes    \n",
        "    self.df = None # Placeholders. Set varabels as none and then update a resut \n",
        "    self.X_train = None\n",
        "    self.X_test = None\n",
        "    self.y_train = None\n",
        "    self.y_test  = None\n",
        "    self.X_train = None\n",
        "    self.X_val = None\n",
        "    self.y_train = None\n",
        "    self.y_val = None\n",
        "    self.cols_standardize = ['rate_time','hpi_time', 'gdp_time']# place for variables to standartize \n",
        "### Pre-processing:\n",
        "  # Read data\n",
        "  def readdf (self, sep=\",\", filename=\"dcr.csv\", cwd = os.getcwd()): # Method whith 3 def attributes: sep - separator, getcwd - path to your working directory\n",
        "    file_name = cwd + \"/\" + filename # Get path of file\n",
        "    data = pd.read_csv(file_name, sep= sep) # Read csv\n",
        "    self.df = data # Save filtered dataset \n",
        "    return self.df # Print dataset\n",
        "  # Clean data\n",
        "  def cleaning (self, data,longformat=True,individual=\"id\", stop=\"time\", stopname=\"start\"): \n",
        "    col1 = data.loc[:,self.cols_standardize]# separate the variables to be standartized\n",
        "    col2 = data.drop(columns=self.cols_standardize)# separate the variables not to be standartized\n",
        "    standard = preprocessing.scale(col1)# standardization of variables\n",
        "    df2 = pd.DataFrame(standard, columns=self.cols_standardize)# to dataframe\n",
        "    df3=pd.concat([col2,df2], axis=1)# merge 2 df\n",
        "    data2 = df3.drop_duplicates() #drop double row entries\n",
        "    self.df = data2 # Save filtered dataset\n",
        "    if longformat:# Bring data into long format (necessary for using the lifeline package's Coxâ€™s time varying proportional hazard model) \n",
        "        data2[stopname] = data2.groupby(individual)[stop].shift(1)\n",
        "        data2[stopname] = data2[stopname].fillna(0)\n",
        "        self.df = data2 # Save filtered dataset\n",
        "        return self.df # Print dataset\n",
        "### Saving and load objects as binary mode\n",
        "  def save (self, dataname, dataframe , cwd = os.getcwd()):# Saving and load objects as binary mode\n",
        "    with open( cwd + '/' + dataname + '.pkl','wb') as path_name: # save df, 'wb' specifies 'write'\n",
        "      pickle.dump(dataframe, path_name)  \n",
        "  def load (self, dataname, cwd = os.getcwd()):# Saving and load objects as binary mode\n",
        "    with open( cwd + '/' + dataname + '.pkl' ,'rb') as path_name:# load df, 'rb' specifies 'read'\n",
        "      dataframe = pickle.load(path_name)\n",
        "      return dataframe # Print dataset\n",
        "### Split the data into 3 sets: train(80%)  + dev (10%) + test(10%)  \n",
        "  def splitdata (self, Xvar, yvar, perc_test=0.1, perc_val=1/9,  shuffle=False):# Method needs 5 arguments: X set with independent vars, y - set w. dependent vars, perc_test=percentage for test set, perc_val=valuation set (=(1-perc_test)*perc_val) \n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = sklearn.model_selection.train_test_split(Xvar, yvar, test_size=perc_test,  shuffle=shuffle)# Split data not randomly to train 90% and test 10%\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = sklearn.model_selection.train_test_split(self.X_train, self.y_train, test_size=perc_val,  shuffle=shuffle) # Split train data not randomly to train 80% and valid 10%\n",
        "    print(len(self.X_train), len(self.X_val), len(self.X_test)) # len - length "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBQbX-7eYpyc"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swNLx_91Lt8h"
      },
      "source": [
        "# temp=Preparation()\n",
        "# newdf = temp.readdf()\n",
        "# temp.cleaning(newdf)\n",
        "# temp.save('qwer', newdf)\n",
        "# www=temp.load('qwer')\n",
        "# temp.splitdata(newdf.loc[:,newdf.columns != \"status_time\"], yvar=newdf.status_time)\n",
        "# temp.X_train"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}